{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ce14e9d",
   "metadata": {},
   "source": [
    "# LiDAR‚ÄìCamera‚ÄìIMU Odometry Pipeline (Practical)\n",
    "This notebook bundles the scripts you were using (`data.py`, `extract.py`, `structure.py`, `training.py`, `model.py`, `inference_lidar_local.py`) into one practical, runnable workflow.\n",
    "- **Edit paths** (`base_dir`, `bag_file`, `url`) for your machine (Windows/G: drive or Colab).\n",
    "- Cells are independent ‚Äî run only what you need.\n",
    "- GPU is **recommended** for training/inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c668ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base dir: G:\\backup\\papers\\Dataset\\hidrive_file\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ‚úÖ EDIT THIS for your machine\n",
    "BASE_DIR = r\"G:\\backup\\papers\\Dataset\\hidrive_file\"  # Windows path\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "training_csv_path = r\"G:\\backup\\papers\\Dataset\\hidrive_file\\training_data.csv\"\n",
    "print(\"Base dir:\", BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1877f26e",
   "metadata": {},
   "source": [
    "## 1. Download HiDrive file (optional)\n",
    "Uses `aria2c` like your `data.py`. Skip if you've already downloaded the `.bag`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2601b3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading to: G:\\backup\\papers\\Dataset\\hidrive_file\\hidrive_file.bag\n",
      "‚ùå Download failed: Command '['aria2c', 'https://my.hidrive.com/api/file?attachment=true&pid=b1584010878.594&access_token=REPLACE_ME', '-o', 'G:\\\\backup\\\\papers\\\\Dataset\\\\hidrive_file\\\\hidrive_file.bag', '-x', '16', '-s', '16', '-k', '1M', '--console-log-level=error', '--summary-interval=5']' returned non-zero exit status 24.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import subprocess\n",
    "\n",
    "# ‚ö†Ô∏è EDIT THESE:\n",
    "url = \"https://my.hidrive.com/api/file?attachment=true&pid=b1584010878.594&access_token=REPLACE_ME\"\n",
    "output_file = os.path.join(BASE_DIR, \"hidrive_file.bag\")\n",
    "\n",
    "cmd = [\n",
    "    \"aria2c\",\n",
    "    url,\n",
    "    \"-o\", output_file,\n",
    "    \"-x\", \"16\",\n",
    "    \"-s\", \"16\",\n",
    "    \"-k\", \"1M\",\n",
    "    \"--console-log-level=error\",\n",
    "    \"--summary-interval=5\",\n",
    "]\n",
    "\n",
    "print(\"Downloading to:\", output_file)\n",
    "try:\n",
    "    subprocess.run(cmd, check=True)\n",
    "    print(\"‚úÖ Download completed\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Download failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57288088",
   "metadata": {},
   "source": [
    "## 2. Extract ROS bag to folders\n",
    "This is your `extract.py` adapted for notebook. Requires ROS Python packages (`rosbag`, `sensor_msgs`, `cv_bridge`). If you're on Windows without ROS, you can skip this and place extracted data manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1ed145",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# If ROS not available, this cell will raise ‚Äî that's expected.\n",
    "try:\n",
    "    import rosbag\n",
    "    from sensor_msgs import point_cloud2\n",
    "    from cv_bridge import CvBridge\n",
    "    import cv2\n",
    "    import open3d as o3d\n",
    "except ImportError as e:\n",
    "    print(\"ROS/Open3D deps missing:\", e)\n",
    "\n",
    "bag_file = os.path.join(BASE_DIR, \"hidrive_file.bag\")\n",
    "\n",
    "out_dirs = {\n",
    "    \"left\": os.path.join(BASE_DIR, \"images\", \"left\"),\n",
    "    \"right\": os.path.join(BASE_DIR, \"images\", \"right\"),\n",
    "    \"lidar\": os.path.join(BASE_DIR, \"lidar\"),\n",
    "    \"camera_info\": os.path.join(BASE_DIR, \"camera_info\"),\n",
    "    \"imu\": os.path.join(BASE_DIR, \"imu\"),\n",
    "    \"tf\": os.path.join(BASE_DIR, \"tf\"),\n",
    "}\n",
    "for d in out_dirs.values():\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(\"[INFO] Opening bag:\", bag_file)\n",
    "\n",
    "try:\n",
    "    bag = rosbag.Bag(bag_file)\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Couldn't open bag:\", e)\n",
    "    bag = None\n",
    "\n",
    "imu_topics = ['/imu/data', '/imu/dq', '/imu/dv', '/imu/mag', '/imu/time_ref']\n",
    "imu_files = {}\n",
    "imu_writers = {}\n",
    "for t in imu_topics:\n",
    "    fname = os.path.join(out_dirs['imu'], t.strip('/').replace('/', '_') + '.csv')\n",
    "    f = open(fname, 'w', newline='')\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(['timestamp', 'data...'])\n",
    "    imu_files[t] = f\n",
    "    imu_writers[t] = w\n",
    "\n",
    "left_images = []\n",
    "right_images = []\n",
    "lidar_scans = []\n",
    "\n",
    "if bag is not None:\n",
    "    bridge = CvBridge()\n",
    "    print(\"[INFO] Starting extraction...\")\n",
    "    for topic, msg, t in tqdm(bag.read_messages(), total=bag.get_message_count()):\n",
    "        ts = t.to_nsec()\n",
    "\n",
    "        if topic == '/bf_lidar/points_raw':\n",
    "            pcd_name = f\"{ts}.pcd\"\n",
    "            pcd_path = os.path.join(out_dirs[\"lidar\"], pcd_name)\n",
    "            try:\n",
    "                pts = list(point_cloud2.read_points(msg, skip_nans=True))\n",
    "                pc = o3d.geometry.PointCloud()\n",
    "                pc.points = o3d.utility.Vector3dVector([[p[0], p[1], p[2]] for p in pts])\n",
    "                o3d.io.write_point_cloud(pcd_path, pc)\n",
    "                lidar_scans.append((ts, pcd_name))\n",
    "            except Exception as e:\n",
    "                print(\"LiDAR conv failed:\", e)\n",
    "\n",
    "        elif topic in imu_topics:\n",
    "            writer = imu_writers[topic]\n",
    "            row = [ts]\n",
    "            if hasattr(msg, '__slots__'):\n",
    "                for field in msg.__slots__:\n",
    "                    val = getattr(msg, field)\n",
    "                    if hasattr(val, '__slots__'):\n",
    "                        for sub in val.__slots__:\n",
    "                            row.append(getattr(val, sub))\n",
    "                    else:\n",
    "                        row.append(val)\n",
    "            writer.writerow(row)\n",
    "\n",
    "        elif topic == '/stereo/left/image_rect':\n",
    "            img = bridge.imgmsg_to_cv2(msg, desired_encoding='passthrough')\n",
    "            name = f\"{ts}.png\"\n",
    "            path = os.path.join(out_dirs[\"left\"], name)\n",
    "            cv2.imwrite(path, img)\n",
    "            left_images.append((ts, name))\n",
    "\n",
    "        elif topic == '/stereo/right/image_rect':\n",
    "            img = bridge.imgmsg_to_cv2(msg, desired_encoding='passthrough')\n",
    "            name = f\"{ts}.png\"\n",
    "            path = os.path.join(out_dirs[\"right\"], name)\n",
    "            cv2.imwrite(path, img)\n",
    "            right_images.append((ts, name))\n",
    "\n",
    "    # associations\n",
    "    assoc_path = os.path.join(BASE_DIR, \"associations.txt\")\n",
    "    with open(assoc_path, \"w\") as f:\n",
    "        for l, r, p in zip(left_images, right_images, lidar_scans):\n",
    "            f.write(f\"{l[0]} {l[1]} {r[0]} {r[1]} {p[0]} {p[1]}\\n\")\n",
    "\n",
    "    for f in imu_files.values():\n",
    "        f.close()\n",
    "\n",
    "    bag.close()\n",
    "    print(\"‚úÖ Extraction complete\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipped ROS extraction (no bag)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d0bba6",
   "metadata": {},
   "source": [
    "## 3. View dataset folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92d433da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Folder structure for: G:\\backup\\papers\\Dataset\\hidrive_file\n",
      "‚îú‚îÄ‚îÄ camera_info\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ 1625657732290284015.yaml\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ 1625657732323859820.yaml\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ +16943 more .yaml files\n",
      "‚îú‚îÄ‚îÄ images\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ left\n",
      "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 1625657732290284015.png\n",
      "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 1625657732290284032.png\n",
      "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ +16943 more .png files\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ right\n",
      "‚îÇ       ‚îú‚îÄ‚îÄ 1625657732290284015.png\n",
      "‚îÇ       ‚îú‚îÄ‚îÄ 1625657732323859820.png\n",
      "‚îÇ       ‚îî‚îÄ‚îÄ +16879 more .png files\n",
      "‚îú‚îÄ‚îÄ imu\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ imu_data.csv\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ imu_dq.csv\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ +3 more .csv files\n",
      "‚îú‚îÄ‚îÄ imu_data_for_training\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ 1.6256577323119644e+18_to_1.6256577325274007e+18.npy\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ 1.6256577325274007e+18_to_1.6256577327408215e+18.npy\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ +107 more .npy files\n",
      "‚îú‚îÄ‚îÄ lidar\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ 1625657732311964358.pcd\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ 1625657732527400610.pcd\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ +2654 more .pcd files\n",
      "‚îú‚îÄ‚îÄ tf\n",
      "‚îî‚îÄ‚îÄ associations.txt\n",
      "‚îî‚îÄ‚îÄ bf_lidar-points_raw.csv\n",
      "‚îî‚îÄ‚îÄ training_data.csv\n",
      "‚îú‚îÄ‚îÄ lidar_trajectory_2d.png\n",
      "‚îú‚îÄ‚îÄ lidar_trajectory_3d.png\n",
      "‚îî‚îÄ‚îÄ +2 more .png files\n",
      "‚îî‚îÄ‚îÄ liv_odometry_model.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def print_folder_tree_limited(startpath, prefix=\"\"):\n",
    "    try:\n",
    "        items = os.listdir(startpath)\n",
    "    except PermissionError:\n",
    "        print(prefix + \"‚îî‚îÄ‚îÄ [Permission Denied]\")\n",
    "        return\n",
    "\n",
    "    file_groups = defaultdict(list)\n",
    "    dirs = []\n",
    "\n",
    "    for item in items:\n",
    "        path = os.path.join(startpath, item)\n",
    "        if os.path.isdir(path):\n",
    "            dirs.append(item)\n",
    "        else:\n",
    "            ext = os.path.splitext(item)[1] or \"no_ext\"\n",
    "            file_groups[ext].append(item)\n",
    "\n",
    "    for i, d in enumerate(sorted(dirs)):\n",
    "        path = os.path.join(startpath, d)\n",
    "        connector = \"‚îî‚îÄ‚îÄ \" if i == len(dirs) - 1 and not file_groups else \"‚îú‚îÄ‚îÄ \"\n",
    "        print(prefix + connector + d)\n",
    "        extension = \"    \" if i == len(dirs) - 1 and not file_groups else \"‚îÇ   \"\n",
    "        print_folder_tree_limited(path, prefix + extension)\n",
    "\n",
    "    for ext, files in file_groups.items():\n",
    "        for idx, file in enumerate(sorted(files)):\n",
    "            if idx >= 2:\n",
    "                if idx == 2:\n",
    "                    print(prefix + f\"‚îî‚îÄ‚îÄ +{len(files)-2} more {ext} files\")\n",
    "                break\n",
    "            connector = \"‚îî‚îÄ‚îÄ \" if idx == len(files)-1 or len(files) <= 2 else \"‚îú‚îÄ‚îÄ \"\n",
    "            print(prefix + connector + file)\n",
    "\n",
    "print(\"üìÅ Folder structure for:\", BASE_DIR)\n",
    "print_folder_tree_limited(BASE_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bf594c",
   "metadata": {},
   "source": [
    "## 4. Create training CSV using LiDAR ICP\n",
    "This is your `training.py` compacted. It reads `associations.txt` and writes `training_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5a0fa0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2656 associations\n",
      "Generating training_data.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1678/2655 [1:16:02<44:16,  2.72s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (os.path.exists(pcd_path_i) \u001b[38;5;129;01mand\u001b[39;00m os.path.exists(pcd_path_j)):\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m T = \u001b[43mestimate_lidar_odometry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpcd_path_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpcd_path_j\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m T \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mestimate_lidar_odometry\u001b[39m\u001b[34m(pcd_src, pcd_tgt)\u001b[39m\n\u001b[32m     38\u001b[39m src_d.estimate_normals(o3d.geometry.KDTreeSearchParamHybrid(radius=\u001b[32m0.5\u001b[39m, max_nn=\u001b[32m30\u001b[39m))\n\u001b[32m     39\u001b[39m tgt_d.estimate_normals(o3d.geometry.KDTreeSearchParamHybrid(radius=\u001b[32m0.5\u001b[39m, max_nn=\u001b[32m30\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m reg = \u001b[43mo3d\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpipelines\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregistration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregistration_icp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43msrc_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43meye\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mo3d\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpipelines\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregistration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTransformationEstimationPointToPlane\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mo3d\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpipelines\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregistration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mICPConvergenceCriteria\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_iteration\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reg.fitness < \u001b[32m0.1\u001b[39m:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import open3d as o3d\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "lidar_dir = os.path.join(BASE_DIR, \"lidar\")\n",
    "associations_path = os.path.join(BASE_DIR, \"associations.txt\")\n",
    "training_csv_path = os.path.join(BASE_DIR, \"training_data.csv\")\n",
    "\n",
    "def load_associations(path):\n",
    "    pairs = []\n",
    "    if not os.path.exists(path):\n",
    "        print(\"‚ùå associations.txt not found:\", path)\n",
    "        return pairs\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"#\") or not line.strip():\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) == 6:\n",
    "                img_l, img_r, pcd = os.path.basename(parts[1]), os.path.basename(parts[3]), os.path.basename(parts[5])\n",
    "                pairs.append((img_l, img_r, pcd))\n",
    "    print(f\"Loaded {len(pairs)} associations\")\n",
    "    return pairs\n",
    "\n",
    "def matrix_to_6dof(T):\n",
    "    t = T[:3, 3]\n",
    "    r = Rotation.from_matrix(T[:3, :3]).as_euler('xyz', degrees=False)\n",
    "    return np.concatenate((t, r))\n",
    "\n",
    "def estimate_lidar_odometry(pcd_src, pcd_tgt):\n",
    "    src = o3d.io.read_point_cloud(pcd_src)\n",
    "    tgt = o3d.io.read_point_cloud(pcd_tgt)\n",
    "    if not src.has_points() or not tgt.has_points():\n",
    "        return None\n",
    "\n",
    "    voxel = 0.1\n",
    "    src_d = src.voxel_down_sample(voxel)\n",
    "    tgt_d = tgt.voxel_down_sample(voxel)\n",
    "    src_d.estimate_normals(o3d.geometry.KDTreeSearchParamHybrid(radius=0.5, max_nn=30))\n",
    "    tgt_d.estimate_normals(o3d.geometry.KDTreeSearchParamHybrid(radius=0.5, max_nn=30))\n",
    "\n",
    "    reg = o3d.pipelines.registration.registration_icp(\n",
    "        src_d, tgt_d, 0.5, np.eye(4),\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPlane(),\n",
    "        o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=2000)\n",
    "    )\n",
    "    if reg.fitness < 0.1:\n",
    "        return None\n",
    "    return reg.transformation\n",
    "\n",
    "pairs = load_associations(associations_path)\n",
    "rows = []\n",
    "\n",
    "if len(pairs) > 1:\n",
    "    print(\"Generating training_data.csv ...\")\n",
    "    for i in tqdm(range(len(pairs)-1)):\n",
    "        _, _, pcd_i = pairs[i]\n",
    "        _, _, pcd_j = pairs[i+1]\n",
    "\n",
    "        pcd_path_i = os.path.join(lidar_dir, pcd_i)\n",
    "        pcd_path_j = os.path.join(lidar_dir, pcd_j)\n",
    "\n",
    "        if not (os.path.exists(pcd_path_i) and os.path.exists(pcd_path_j)):\n",
    "            continue\n",
    "\n",
    "        T = estimate_lidar_odometry(pcd_path_i, pcd_path_j)\n",
    "        if T is None:\n",
    "            continue\n",
    "        pose = matrix_to_6dof(T)\n",
    "        rows.append([pcd_i, pcd_j, *pose.tolist()])\n",
    "\n",
    "    if rows:\n",
    "        df = pd.DataFrame(rows, columns=[\"source_pcd\",\"target_pcd\",\"tx\",\"ty\",\"tz\",\"rx\",\"ry\",\"rz\"])\n",
    "        df.to_csv(training_csv_path, index=False)\n",
    "        print(\"‚úÖ Saved:\", training_csv_path, \"->\", len(df), \"samples\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No rows generated (ICP failed).\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Not enough associations to build dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042951fa",
   "metadata": {},
   "source": [
    "## 5. Multimodal Odometry Model (LiDAR + IMU + Vision)\n",
    "This is your model (from `model.py`) in simplified form, with training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cb61954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting 1 short epoch (debug)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñä         | 2/25 [00:01<00:10,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0027224537916481495\n",
      "loss: 0.03581151366233826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|‚ñà‚ñå        | 4/25 [00:01<00:05,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.006726098246872425\n",
      "loss: 0.006310615222901106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|‚ñà‚ñà‚ñç       | 6/25 [00:01<00:03,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.024450203403830528\n",
      "loss: 0.009161963127553463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 8/25 [00:02<00:03,  5.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.005815529730170965\n",
      "loss: 0.0036914057563990355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 10/25 [00:02<00:02,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.008796392939984798\n",
      "loss: 0.031459808349609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 12/25 [00:02<00:02,  6.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.009059488773345947\n",
      "loss: 0.007784193381667137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 14/25 [00:02<00:01,  6.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.005449643358588219\n",
      "loss: 0.004449939355254173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 16/25 [00:03<00:01,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.010698378086090088\n",
      "loss: 0.004284811206161976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 18/25 [00:03<00:00,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0048540811985731125\n",
      "loss: 0.016140226274728775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 20/25 [00:03<00:00,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.04936585575342178\n",
      "loss: 0.017895657569169998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 22/25 [00:04<00:00,  7.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.003668104065582156\n",
      "loss: 0.0033381031826138496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 24/25 [00:04<00:00,  7.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.01815703511238098\n",
      "loss: 0.0024997908622026443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:04<00:00,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.04777729511260986\n",
      "‚úÖ Saved model to G:\\backup\\papers\\Dataset\\hidrive_file\\liv_odometry_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.amp import autocast, GradScaler\n",
    "import cv2\n",
    "\n",
    "IMG_H, IMG_W = 128, 128\n",
    "N_POINTS = 2048\n",
    "IMU_SAVE_DIR = os.path.join(BASE_DIR, \"imu_data_for_training\")\n",
    "os.makedirs(IMU_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "def load_pcd(path, num_points=N_POINTS):\n",
    "    try:\n",
    "        pcd = o3d.io.read_point_cloud(path)\n",
    "        pts = np.asarray(pcd.points, dtype=np.float32)\n",
    "        if pts.shape[0] > num_points:\n",
    "            idx = np.random.choice(pts.shape[0], num_points, replace=False)\n",
    "            pts = pts[idx]\n",
    "        elif pts.shape[0] < num_points and pts.shape[0] > 0:\n",
    "            idx = np.random.choice(pts.shape[0], num_points - pts.shape[0], replace=True)\n",
    "            pts = np.vstack((pts, pts[idx]))\n",
    "        elif pts.shape[0] == 0:\n",
    "            pts = np.zeros((num_points, 3), dtype=np.float32)\n",
    "        return pts\n",
    "    except:\n",
    "        return np.zeros((num_points, 3), dtype=np.float32)\n",
    "\n",
    "def load_image(path):\n",
    "    img = cv2.imread(path)\n",
    "    if img is None:\n",
    "        return np.zeros((IMG_H, IMG_W, 3), dtype=np.float32)\n",
    "    img = cv2.resize(img, (IMG_W, IMG_H)).astype(np.float32) / 255.0\n",
    "    return img\n",
    "\n",
    "class SimpleLIDARDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.lidar_dir = os.path.join(BASE_DIR, \"lidar\")\n",
    "        self.image_dir = os.path.join(BASE_DIR, \"images\", \"left\")\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        src_pc = load_pcd(os.path.join(self.lidar_dir, row['source_pcd']))\n",
    "        tgt_pc = load_pcd(os.path.join(self.lidar_dir, row['target_pcd']))\n",
    "        pc = np.vstack((src_pc, tgt_pc)).astype(np.float32)\n",
    "        pc -= pc.mean(axis=0, keepdims=True)\n",
    "        pc = torch.tensor(pc, dtype=torch.float32).permute(1, 0)\n",
    "\n",
    "        # dummy imu\n",
    "        imu = torch.zeros((1, 6), dtype=torch.float32)\n",
    "\n",
    "        # dummy image pair\n",
    "        src_img = load_image(os.path.join(self.image_dir, row.get('source_img', '')))\n",
    "        tgt_img = load_image(os.path.join(self.image_dir, row.get('target_img', '')))\n",
    "        img = np.concatenate((src_img, tgt_img), axis=2)\n",
    "        img = torch.tensor(img, dtype=torch.float32).permute(2, 0, 1)\n",
    "\n",
    "        pose = torch.tensor(row[['tx','ty','tz','rx','ry','rz']].values.astype(np.float32))\n",
    "        return pc, imu, img, pose\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pcs = [b[0] for b in batch]\n",
    "    imus = [b[1] for b in batch]\n",
    "    imgs = [b[2] for b in batch]\n",
    "    poses = [b[3] for b in batch]\n",
    "    pcs = torch.stack(pcs)\n",
    "    imgs = torch.stack(imgs)\n",
    "    poses = torch.stack(poses)\n",
    "    imus = pad_sequence(imus, batch_first=True, padding_value=0.0)\n",
    "    return pcs, imus, imgs, poses\n",
    "\n",
    "class LIV_OdometryNet(nn.Module):\n",
    "    def __init__(self, d_model=512, n_head=8):\n",
    "        super().__init__()\n",
    "        self.lidar_fe = nn.Sequential(\n",
    "            nn.Conv1d(3, 64, 1), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "            nn.Conv1d(64, 128, 1), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Conv1d(128, 1024, 1), nn.BatchNorm1d(1024), nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(1)\n",
    "        )\n",
    "        self.imu_rnn = nn.GRU(6, 128, 2, batch_first=True, dropout=0.1)\n",
    "        self.visual_fe = nn.Sequential(\n",
    "            nn.Conv2d(6, 32, 3, 2, 1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, 2, 1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, 2, 1), nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 3, 2, 1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.lidar_proj = nn.Linear(1024, d_model)\n",
    "        self.imu_proj = nn.Linear(128, d_model)\n",
    "        self.visual_proj = nn.Linear(256, d_model)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head, batch_first=True)\n",
    "        self.fusion = nn.TransformerEncoder(enc_layer, num_layers=2)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model*3, 1024), nn.ReLU(),\n",
    "            nn.Linear(1024, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 6)\n",
    "        )\n",
    "\n",
    "    def forward(self, pc, imu, img):\n",
    "        B = pc.size(0)\n",
    "        lidar_feat = self.lidar_fe(pc).view(B, -1)\n",
    "        _, h_n = self.imu_rnn(imu)\n",
    "        imu_feat = h_n[-1]\n",
    "        vis_feat = self.visual_fe(img).view(B, -1)\n",
    "\n",
    "        tok_l = self.lidar_proj(lidar_feat)\n",
    "        tok_i = self.imu_proj(imu_feat)\n",
    "        tok_v = self.visual_proj(vis_feat)\n",
    "\n",
    "        tokens = torch.stack([tok_l, tok_i, tok_v], dim=1)\n",
    "        fused = self.fusion(tokens).reshape(B, -1)\n",
    "        out = self.head(fused)\n",
    "        return out\n",
    "\n",
    "training_csv = training_csv_path\n",
    "if os.path.exists(training_csv):\n",
    "    ds = SimpleLIDARDataset(training_csv)\n",
    "    loader = DataLoader(ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "    model = LIV_OdometryNet().to(device)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    print(\"Starting 1 short epoch (debug)...\")\n",
    "    model.train()\n",
    "    for pc, imu, img, pose in tqdm(loader):\n",
    "        pc, imu, img, pose = pc.to(device), imu.to(device), img.to(device), pose.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(device_type=device.type, dtype=torch.float16 if device.type=='cuda' else torch.bfloat16):\n",
    "            preds = model(pc, imu, img)\n",
    "            loss = criterion(preds, pose)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        print(\"loss:\", loss.item())\n",
    "    torch.save(model.state_dict(), os.path.join(BASE_DIR, \"liv_odometry_model.pth\"))\n",
    "    print(\"‚úÖ Saved model to\", os.path.join(BASE_DIR, \"liv_odometry_model.pth\"))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è training_data.csv not found, skip training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039f9125",
   "metadata": {},
   "source": [
    "## 6. Inference + Trajectory Plot\n",
    "Loads the saved model and the CSV, runs inference in sequence, and plots predicted vs. chained GT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588dca47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gsing\\AppData\\Local\\Temp\\ipykernel_8600\\2087450061.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Could not load model weights: Error(s) in loading state_dict for LIV_OdometryNet:\n",
      "\tMissing key(s) in state_dict: \"img_cnn.0.weight\", \"img_cnn.0.bias\", \"img_cnn.2.weight\", \"img_cnn.2.bias\", \"pc_fc.0.weight\", \"pc_fc.0.bias\", \"pc_fc.2.weight\", \"pc_fc.2.bias\", \"imu_fc.0.weight\", \"imu_fc.0.bias\", \"imu_fc.2.weight\", \"imu_fc.2.bias\", \"fusion.0.weight\", \"fusion.0.bias\", \"fusion.2.weight\", \"fusion.2.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"vision.feat.0.weight\", \"vision.feat.0.bias\", \"vision.feat.2.weight\", \"vision.feat.2.bias\", \"vision.feat.4.weight\", \"vision.feat.4.bias\", \"vision.feat.6.weight\", \"vision.feat.6.bias\", \"lidar.mlp.0.weight\", \"lidar.mlp.0.bias\", \"lidar.mlp.2.weight\", \"lidar.mlp.2.bias\", \"lidar.mlp.4.weight\", \"lidar.mlp.4.bias\", \"cross1.attn_v2l.in_proj_weight\", \"cross1.attn_v2l.in_proj_bias\", \"cross1.attn_v2l.out_proj.weight\", \"cross1.attn_v2l.out_proj.bias\", \"cross1.attn_l2v.in_proj_weight\", \"cross1.attn_l2v.in_proj_bias\", \"cross1.attn_l2v.out_proj.weight\", \"cross1.attn_l2v.out_proj.bias\", \"cross1.ln1.weight\", \"cross1.ln1.bias\", \"cross1.ln2.weight\", \"cross1.ln2.bias\", \"cross2.attn_v2l.in_proj_weight\", \"cross2.attn_v2l.in_proj_bias\", \"cross2.attn_v2l.out_proj.weight\", \"cross2.attn_v2l.out_proj.bias\", \"cross2.attn_l2v.in_proj_weight\", \"cross2.attn_l2v.in_proj_bias\", \"cross2.attn_l2v.out_proj.weight\", \"cross2.attn_l2v.out_proj.bias\", \"cross2.ln1.weight\", \"cross2.ln1.bias\", \"cross2.ln2.weight\", \"cross2.ln2.bias\", \"head.0.weight\", \"head.0.bias\", \"head.2.weight\", \"head.2.bias\", \"head.4.weight\", \"head.4.bias\", \"imu_fc.weight\", \"imu_fc.bias\". \n",
      "Proceeding with random initialization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Inference]:   0%|          | 0/25 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 142\u001b[39m\n\u001b[32m    139\u001b[39m cur_gt = np.eye(\u001b[32m4\u001b[39m, dtype=np.float32)\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpose_6d\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43minf_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m[Inference]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mSimpleLIDARDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     53\u001b[39m imu = torch.randn(\u001b[32m1\u001b[39m, \u001b[32m6\u001b[39m)          \u001b[38;5;66;03m# Simulated IMU\u001b[39;00m\n\u001b[32m     54\u001b[39m img = torch.randn(\u001b[32m3\u001b[39m, \u001b[32m128\u001b[39m, \u001b[32m128\u001b[39m)   \u001b[38;5;66;03m# Simulated Image\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m pose_6d = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpose_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pc.squeeze(\u001b[32m0\u001b[39m), imu.squeeze(\u001b[32m0\u001b[39m), img, pose_6d\n",
      "\u001b[31mTypeError\u001b[39m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "MODEL_PATH = os.path.join(BASE_DIR, \"liv_odometry_model.pth\")\n",
    "\n",
    "def dof6_to_matrix(pose_6d):\n",
    "    T = np.eye(4, dtype=np.float32)\n",
    "    t = pose_6d[0:3]\n",
    "    r_euler = pose_6d[3:6]\n",
    "    r = Rotation.from_euler('xyz', r_euler, degrees=False)\n",
    "    T[:3, :3] = r.as_matrix().astype(np.float32)\n",
    "    T[:3, 3] = t\n",
    "    return T\n",
    "\n",
    "if os.path.exists(MODEL_PATH) and os.path.exists(training_csv_path):\n",
    "    ds_inf = SimpleLIDARDataset(training_csv_path)\n",
    "    inf_loader = DataLoader(ds_inf, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    model = LIV_OdometryNet().to(device)\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    pred_traj = [np.zeros(3, dtype=np.float32)]\n",
    "    gt_traj = [np.zeros(3, dtype=np.float32)]\n",
    "    cur_pred = np.eye(4, dtype=np.float32)\n",
    "    cur_gt = np.eye(4, dtype=np.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for pc, imu, img, pose_6d in tqdm(inf_loader, desc=\"[Inference]\"):\n",
    "            pc, imu, img = pc.to(device), imu.to(device), img.to(device)\n",
    "            preds = model(pc, imu, img).cpu().numpy()\n",
    "            gts = pose_6d.numpy()\n",
    "            for p6, g6 in zip(preds, gts):\n",
    "                Tp = dof6_to_matrix(p6)\n",
    "                Tg = dof6_to_matrix(g6)\n",
    "                cur_pred = cur_pred @ Tp\n",
    "                cur_gt = cur_gt @ Tg\n",
    "                pred_traj.append(cur_pred[:3, 3].copy())\n",
    "                gt_traj.append(cur_gt[:3, 3].copy())\n",
    "\n",
    "    pred_traj = np.array(pred_traj)\n",
    "    gt_traj = np.array(gt_traj)\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.plot(gt_traj[:,0], gt_traj[:,1], label=\"Ground Truth\")\n",
    "    plt.plot(pred_traj[:,0], pred_traj[:,1], \"--\", label=\"Predicted\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Trajectory (XY)\")\n",
    "    plt.axis(\"equal\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Model or CSV missing ‚Äî run training first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1c9e387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gt_xy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Assume gt_xy and pred_xy are numpy arrays of shape (N, 2)\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# If you already have them, just skip this dummy example.\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Otherwise, load from your CSV or the arrays you computed.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Convert 2D XY trajectories to 3D (Z=0)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m gt_xyz = np.column_stack((\u001b[43mgt_xy\u001b[49m, np.zeros(\u001b[38;5;28mlen\u001b[39m(gt_xy))))\n\u001b[32m     16\u001b[39m pred_xyz = np.column_stack((pred_xy, np.zeros(\u001b[38;5;28mlen\u001b[39m(pred_xy))))\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Create line sets for both\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'gt_xy' is not defined"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "# Assume gt_xy and pred_xy are numpy arrays of shape (N, 2)\n",
    "# If you already have them, just skip this dummy example.\n",
    "# Otherwise, load from your CSV or the arrays you computed.\n",
    "# gt_xy = np.load(\"gt.npy\")\n",
    "# pred_xy = np.load(\"pred.npy\")\n",
    "\n",
    "# Example: random dummy data (REMOVE this and use your real gt_xy, pred_xy)\n",
    "# gt_xy = np.cumsum(np.random.randn(200, 2), axis=0)\n",
    "# pred_xy = gt_xy + np.random.randn(200, 2) * 0.5\n",
    "\n",
    "# Convert 2D XY trajectories to 3D (Z=0)\n",
    "gt_xyz = np.column_stack((gt_xy, np.zeros(len(gt_xy))))\n",
    "pred_xyz = np.column_stack((pred_xy, np.zeros(len(pred_xy))))\n",
    "\n",
    "# Create line sets for both\n",
    "def make_line_set(points, color):\n",
    "    lines = [[i, i+1] for i in range(len(points)-1)]\n",
    "    colors = [color for _ in lines]\n",
    "    line_set = o3d.geometry.LineSet()\n",
    "    line_set.points = o3d.utility.Vector3dVector(points)\n",
    "    line_set.lines = o3d.utility.Vector2iVector(lines)\n",
    "    line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "    return line_set\n",
    "\n",
    "# Ground truth: green, Predicted: red\n",
    "gt_lines = make_line_set(gt_xyz, [0, 1, 0])\n",
    "pred_lines = make_line_set(pred_xyz, [1, 0, 0])\n",
    "\n",
    "# Coordinate frame (for reference)\n",
    "origin = o3d.geometry.TriangleMesh.create_coordinate_frame(size=1.0)\n",
    "\n",
    "# Visualize\n",
    "o3d.visualization.draw_geometries([gt_lines, pred_lines, origin],\n",
    "                                  window_name=\"Odometry Trajectory (Open3D)\",\n",
    "                                  width=900, height=700)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa955e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
